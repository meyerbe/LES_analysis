import netCDF4 as nc
import argparse
import os
import json as  simplejson
import pylab as plt
from matplotlib.colors import LogNorm

import numpy as np
import itertools
from scipy import linalg
from sklearn import mixture


label_size = 8
plt.rcParams['xtick.labelsize'] = label_size
plt.rcParams['ytick.labelsize'] = label_size


'''
Gaussian Mixture Model = Superposition of multiple Gaussian Distributions
(1) Fit A GMM to data points to get means, covariance matrices and relaive weights
    --> use the expectation-maximization algorithm
(2) check out to which mode each data point belongs (class probability)

http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture
The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.
- GaussianMixture.fit method: learns a Gaussian Mixture Model from train data.
- GaussianMixture.predict: Given test data, it can assign to each sample the Gaussian it mostly probably belong to.
- different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.

 First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed
  around the origin) and computes for each point a probability of being generated by each component of the model.
  Then, one tweaks the parameters to maximize the likelihood of the data given those assignments.
  Repeating this process is guaranteed to always converge to a local optimum.

BIC: Bayesian Information Criterion

sklearn.mixture.GaussianMixture(n_compontens=1,covariance_type='full',
tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,
init_params='kmeans',weights_init=None,mans_init=None,precisions_init=None)


Methods:
- aic
- bic
- get_params
- predict(X[, y]): Predict the labels for the data samples in X using trained model.
- predict_proba
- sample
- score
- score_samples(X): Generate random samples from the fitted Gaussian distribution.
- set_params

'''

def main():
    parser = argparse.ArgumentParser(prog='PyCLES')
    parser.add_argument("path")
    args = parser.parse_args()

    case_name = 'Bomex'
    nml = simplejson.loads(open(args.path + case_name + '.in').read())
    dx = nml['grid']['dx']
    dy = nml['grid']['dy']
    dz = nml['grid']['dz']
    global nx, ny, nz, ntot
    nx = nml['grid']['nx']
    ny = nml['grid']['ny']
    nz = nml['grid']['nz']
    ntot = nx*ny*nz
    print('nx,ny,nz; ntot:', nx, ny, nz, ntot)
    global dt
    # dt = np.int(args.time2) - np.int(args.time1)
    # print('dt', dt)
    global fullpath_out
    fullpath_out = args.path


    global time
    time = np.zeros((1))
    # for d in files:
    #     time[i] = d[0:-3]

    files = os.listdir(os.path.join(args.path,'fields'))
    N = len(files)
    print('Found the following directories', files, N)
    for d in files:
        time = np.sort(np.append(time, np.int(d[0:-3])))
    # print(time)

    '''
    (1) uni-variate PDF for single variable
    (2) multi-variate PDF for (s,qt,w)
    '''
    for var in ['s']:
        i = 0
        for d in files:
            # print('t summing ' + d)
            fullpath_in = os.path.join(args.path, 'fields', d)
            # print(fullpath_in)

            data = read_in_netcdf_fields(var,fullpath_in)

    #         # i += 1
    #
    # Gaussian_mixture(data)

        # plot1D(var, sum, max, average, time_av)

    Gaussian_mixture_example_1D()
    # Gaussian_mixture_example_2D()
    # Gaussian_mixture_example2()


    return


#----------------------------------------------------------------------
def Gaussian_mixture(data_):
    print(data_.shape, (nx*ny), nz)
    data = data_.reshape((nx*ny), nz)
    print(data.shape)

    for i in range(0):
        clf.mixture.GaussianMixture(n_components=2,covariance_type='full')

    # lowest_bic = np.infty
    # bic = []
    # n_components_range = range(1, 7)
    # # cv_types = ['spherical', 'tied', 'diag', 'full']      # problem for 'tied'
    # cv_types = ['spherical', 'diag', 'full']
    # for cv_type in cv_types:
    #     print('cv type: ', cv_type)
    #     for n_components in n_components_range:
    #         print('n comp. ', n_components)
    #         '''mixture.GMM'''
    #         gmm = mixture.GaussianMixture(n_components=n_components,
    #                                       covariance_type=cv_type)
    #         '''fit'''
    #         gmm.fit(data)
    #         '''use BIC for avoiding under-/overfitting data'''
    #         bic.append(gmm.bic(data))
    #         if bic[-1] < lowest_bic:
    #             lowest_bic = bic[-1]
    #             best_gmm = gmm
    # bic = np.array(bic)  # convert list to array
    return


def Gaussian_mixture_example_1D():
    print('Gaussian mixture example 1D')
    n_samples = 100
    np.random.seed(0)
    A = np.random.randn(n_samples).reshape(n_samples,1)
    print(A)
    B, bin_edges = np.histogram(A,bins=10)
    # print(B)
    # print(bin_edges)

    clf_1 = mixture.GaussianMixture(n_components=1,covariance_type='full')
    clf_1.fit(A)
    D_1 = clf_1.sample(n_samples=10)
    # print(type(D), D)
    # print(D[0])
    # plt.plot(D[0],'-x')
    clf_2 = mixture.GaussianMixture(n_components=2, covariance_type='full')
    clf_2.fit(A)
    D_2 = clf_2.sample(n_samples=10)

    n = 41
    x = np.linspace(-3,3,n).reshape(n,1)
    E_1 = -clf_1.score_samples(x)
    E_2 = -clf_2.score_samples(x)
    F_1 = clf_1.score(x)        # log-likelihood for whole sample
    F_2 = clf_2.score(x)
    print('F_1', np.exp(F_1))

    plt.subplot(3,1,1)
    plt.hist(A)
    plt.subplot(3,1,2)
    plt.plot(x, np.exp(-E_1), label='1 comp')
    plt.plot(x, np.exp(-E_2), label='2 comp')
    # plt.ylim([-0,1])
    plt.subplot(3,1,3)
    # plt.plot(x,F_1,label='1 comp')
    # plt.plot(x,F_2, label='2 comp')
    plt.plot(x, E_1, label='1 comp')
    plt.plot(x, E_2, label='2 comp')
    plt.yscale('log')
    plt.legend(loc=4)
    plt.show()

    print('end')

    return

# http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py
def Gaussian_mixture_example_2D():
    print('Gaussian mixture example 2D')
    # (1) generate random sample (two gaussian PDF) as training data
    n_samples = 300
    np.random.seed(0)
    A = np.random.randn(n_samples,2) + np.array([20,20])     # shift gaussian data
    aux = np.array([[0.,-0.7],[3.5,.7]])
    B = np.dot(np.random.randn(n_samples, 2), aux)              # stretched gaussian data
    C = np.vstack([A,B])            # C.shape=(600,2)
    print('C', C.shape)

    # (2) fit GMM with two components
    '''mixture.GaussianMixture.fit'''
    clf = mixture.GaussianMixture(n_components=2, covariance_type='full')
    clf.fit(C)

    # (3) displace model predictions (evaluate EM-PDF)
    '''mixture.GaussianMixture.score_samples'''
    x = np.linspace(-20.,30.)
    y = np.linspace(-20.,40.)
    X,Y = np.meshgrid(x,y)
    XY = np.array([X.ravel(),Y.ravel()]).T
    Z = -clf.score_samples(XY)
    Z = Z.reshape(X.shape)

    plt.scatter(C[:, 0], C[:, 1], .8)  # training data
    CS = plt.contour(X,Y,Z,norm=LogNorm(vmin=1.0,vmax=1000.0),levels=np.logspace(0,3,10))   # model prediction

    plt.colorbar(CS, shrink=0.8, extend='both')
    plt.title('Negative log-likelihood predicted by a GMM')
    plt.axis('tight')

    plt.show()

    return



# iterate over different number of components
def Gaussian_mixture_example2():
    # Number of samples per component
    n_samples = 500

    np.random.seed(0)
    C = np.array([[0.,-0.1],[1.7,0.4]])
    print(C)
    '''numpy.r_: Translates slice objects to concatenation along the first axis.'''
    b = np.random.randn(n_samples,2)
    v1 = np.dot(b,C)    # dot product
    print('b', b.shape, C.shape, v1.shape)
    d = 0.7*np.random.randn(n_samples,2)
    e = np.array([-6,3])
    v2 = d + e
    print(d.shape, e.shape, v2.shape)
    X = np.r_[v1,v2]    # concatenate arrays

    lowest_bic = np.infty
    bic = []
    n_components_range = range(1,7)     # number of gaussians in superposition
    cv_types = ['spherical', 'tied', 'diag', 'full']
    for cv_type in cv_types:
        for n_components in n_components_range:
            '''mixture.GaussianMixture'''
            gmm = mixture.GaussianMixture(n_components=n_components,
                                          covariance_type=cv_type)
            '''GaussianMixture.fit'''
            gmm.fit(X)
            '''GaussianMixture.gmm: use BIC for avoiding under-/overfitting data'''
            bic.append(gmm.bic(X))
            if bic[-1] < lowest_bic:
                lowest_bic = bic[-1]
                best_gmm = gmm
    bic = np.array(bic)     # convert list to array

    bars = []
    # plotting
    pl1 = plt.subplot(2,1,1)
    plt.xticks(n_components_range)

    pl2 = plt.subplot(2,1,2)
    '''GaussianMixture.predict'''
    clf = best_gmm
    Y_ = clf.predict(X)
    m1 = clf.means_
    m2 = clf.covariances_
    plt.xticks(())      # no xticks and tick-labels
    plt.yticks(())

    plt.show()




    return
#----------------------------------------------------------------------
def plot1D(var_name, sum_, max_, average_, time_av_):
    global ntot, time
    print('time', time)
    plt.figure(figsize=(16,5))
    plt.subplot(1,4,1)
    plt.plot(sum_/ntot,linewidth=3,label=r'sum/n$_{tot}$')
    plt.plot(average_,'x',label='average')
    plt.plot(max_,linewidth=3,label='max')
    plt.title(var_name + ': (average, maximum)')
    plt.legend(loc=2,fontsize=10)
    ax = plt.gca()
    ax.tick_params(direction='out', pad=0)
    labels = ax.get_xticks()
    for i in range(int(labels.shape[0])):
        labels[i] = time[i + 1]
    ax.set_xticklabels(labels, fontsize=8)
    plt.xlabel('time [s]')

    plt.subplot(1, 4, 2)
    # plt.plot(sum_)
    # plt.plot((sum_-average_*ntot),'r',linewidth=3,label='deviation')
    plt.plot(sum_,linewidth=3,label='sum')
    plt.plot(time_av_*np.ones(sum_.shape[0]),'-x', linewidth=3,label=r'<sum>$_t$')
    # plt.plot(sum_/average_,linewidth=3,label='sum')
    # plt.plot(ntot*np.ones(sum_.shape[0]),'-x', linewidth=3,label='average*Ntot')
    plt.legend(loc=2,fontsize=10)
    plt.title(var_name+r': sum vs. <sum>$_t$')
    ax = plt.gca()
    ax.tick_params(direction='out', pad=0)
    labels = ax.get_xticks()
    for i in range(int(labels.shape[0])):
        labels[i] = int(time[i + 1])
    ax.set_xticklabels(labels, fontsize=8)
    plt.xlabel('time [s]')

    plt.subplot(1, 4, 3)
    plt.plot(sum_/ntot, linewidth=3, label=r'sum/n$_{tot}$')
    plt.plot(time_av_/ntot * np.ones(sum_.shape[0]), '-x', linewidth=3, label=r'<sum/n$_{tot}$>$_t$')
    plt.legend(loc=2,fontsize=10)
    plt.title(var_name + r': sum/n$_{tot}$ vs. <sum/n$_{tot}$>$_t$')
    ax = plt.gca()
    ax.tick_params(direction='out', pad=0)
    labels = ax.get_xticks()
    for i in range(int(labels.shape[0])):
        labels[i] = int(time[i + 1])
    ax.set_xticklabels(labels, fontsize=8)
    plt.xlabel('time [s]')

    plt.subplot(1, 4,4)
    plt.plot((sum_-sum_[0])/sum_[0]*100, linewidth=3, label=r'(sum-sum[t$_0$])/sum[t$_0$]*100')
    plt.legend(loc=2, fontsize=10)
    plt.title(var_name + r': $\Delta$sum [%]')
    ax = plt.gca()
    ax.tick_params(direction='out', pad=0)
    labels = ax.get_xticks()
    for i in range(int(labels.shape[0])):
        labels[i] = int(time[i + 1])
    ax.set_xticklabels(labels, fontsize=8)
    plt.xlabel('time [s]')
    # plt.show()
    plt.savefig(fullpath_out + 'conservation_' + var_name + '.png')
    plt.close()
    return

#----------------------------------------------------------------------
def test_compatibility(data1,data2):
    #    global nx
    phimax = -9999.99
    phimin = 9999.99
    for i in xrange(1,nx-1):
        print(i)
        for j in xrange(1,ny-1):
            for k in xrange(1,nz-1):
                for di in xrange(-1,2):
                    for dj in xrange(-1,2):
                        for dk in xrange(-1,2):
                            a = data1[i+di,j+dj,k+dk]
                            if a<phimin:
                                phimin = a
                            elif a>max:
                                phimax = a
                if data2[i,j,k] < phimin:
                    print('problem min')
                elif data2[i,j,k] > phimax:
                    print('problem max')

    #phimin = min(data1[i+di,j+dj,k+dk])
    return

#----------------------------------------------------------------------
def read_in_netcdf_fields(variable_name, fullpath_in):
    rootgrp = nc.Dataset(fullpath_in, 'r')
    var = rootgrp.groups['fields'].variables[variable_name]
    
    shape = var.shape
    # print('shape:',var.shape)
    data = np.ndarray(shape = var.shape)
    data = var[:]
    rootgrp.close()
    return data

#----------------------------------------------------------------------
def read_in(variable_name, group_name, fullpath_in):
    f = File(fullpath_in)
    
    #Get access to the profiles group
    profiles_group = f[group_name]
    #Get access to the variable dataset
    variable_dataset = profiles_group[variable_name]
    #Get the current shape of the dataset
    variable_dataset_shape = variable_dataset.shape
    
    variable = np.ndarray(shape = variable_dataset_shape)
    for t in range(variable_dataset_shape[0]):
        if group_name == "timeseries":
            variable[t] = variable_dataset[t]
        elif group_name == "profiles":
            variable[t,:] = variable_dataset[t, :]
        elif group_name == "correlations":
            variable[t,:] = variable_dataset[t, :]
        elif group_name == "fields":
            variable[t] = variable_dataset[t]

    f.close()
    return variable




if __name__ == "__main__":
    main()