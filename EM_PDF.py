import netCDF4 as nc
import argparse
import os
import json as  simplejson
import pylab as plt
from matplotlib.colors import LogNorm

import numpy as np
import itertools
from scipy import linalg
from sklearn import mixture


label_size = 8
plt.rcParams['xtick.labelsize'] = label_size
plt.rcParams['ytick.labelsize'] = label_size


'''
Gaussian Mixture Model = Superposition of multiple Gaussian Distributions
(1) Fit A GMM to data points to get means, covariance matrices and relaive weights
    --> use the expectation-maximization algorithm
(2) check out to which mode each data point belongs (class probability)

http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture
The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.
- GaussianMixture.fit method: learns a Gaussian Mixture Model from train data.
- GaussianMixture.predict: Given test data, it can assign to each sample the Gaussian it mostly probably belong to.
- different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.

 First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed
  around the origin) and computes for each point a probability of being generated by each component of the model.
  Then, one tweaks the parameters to maximize the likelihood of the data given those assignments.
  Repeating this process is guaranteed to always converge to a local optimum.

BIC: Bayesian Information Criterion

sklearn.mixture.GaussianMixture(n_compontens=1,covariance_type='full',
tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,
init_params='kmeans',weights_init=None,mans_init=None,precisions_init=None)


Methods:
- aic
- bic
- get_params
- predict(X[, y]): Predict the labels for the data samples in X using trained model.
- predict_proba
- sample
- score
- score_samples(X): Generate random samples from the fitted Gaussian distribution.
- set_params

'''

def main():
    parser = argparse.ArgumentParser(prog='PyCLES')
    parser.add_argument("path")
    args = parser.parse_args()

    case_name = 'Bomex'
    nml = simplejson.loads(open(args.path + case_name + '.in').read())
    global dz
    dx = nml['grid']['dx']
    dy = nml['grid']['dy']
    dz = nml['grid']['dz']
    global nx, ny, nz, ntot
    nx = nml['grid']['nx']
    ny = nml['grid']['ny']
    nz = nml['grid']['nz']
    ntot = nx*ny*nz
    print('nx,ny,nz; ntot:', nx, ny, nz, ntot)
    global dt
    # dt = np.int(args.time2) - np.int(args.time1)
    # print('dt', dt)
    global fullpath_out
    fullpath_out = args.path


    global time
    time = np.zeros((1))
    # for d in files:
    #     time[i] = d[0:-3]

    files = os.listdir(os.path.join(args.path,'fields'))
    N = len(files)
    print('Found the following directories', files, N)
    for d in files:
        time = np.sort(np.append(time, np.int(d[0:-3])))
    # print(time)

    # (5) IO
    # (a) create file for eddy fields
    var_list = ['w','s','qt']
    for d in files:
        time = np.int(d[0:-3])
        nc_file_name = 'EM2_' + str(time)
        create_statistics_file(fullpath_out, nc_file_name)

    '''
    (1) uni-variate PDF for single variable
    '''
    data = np.ndarray(shape=((nx*ny),1))
    zrange= range(0,30,10)
    # means = np.ndarray(shape=(np.len(varlist)))
    for var in var_list:
        i = 0
        for d in files:
            t = np.int(d[0:-3])
            # print('t summing ' + d)
            fullpath_in = os.path.join(args.path, 'fields', d)
            print(fullpath_in)
            data_ = read_in_netcdf_fields(var,fullpath_in).reshape((nx*ny),nz)
            for i in zrange:
                data[:,0] = data_[:,i]
                means, covariance = Gaussian_mixture_univariate(data, var, t, i*dz)

                # # (b) dump eddy fields
                # #    add_field(os.path.join(out_path,nc_file_name+'.nc'), var_name)
                # dump_variables(os.path.join(out_path, nc_file_name + '.nc'), 'means', u_eddy)
                # dump_variables(os.path.join(out_path, nc_file_name + '.nc'), 'v_eddy', v_eddy)
                # dump_variables(os.path.join(out_path, nc_file_name + '.nc'), 'w_eddy', w_eddy)
                # dump_variables(os.path.join(out_path, nc_file_name + '.nc'), 'phi_eddy', phi_eddy)

    '''
    (2) multi-variate PDF for (s,qt,w)
    '''
    data = np.ndarray(shape=((nx*ny),2))
    zrange= range(0,np.int(nz*dz), np.int(5*dz))
    print('zrange', zrange)
    zrange = np.asarray(zrange)/dz
    print('zrange', zrange)
    for d in files:
        fullpath_in = os.path.join(args.path, 'fields', d)
        print(fullpath_in)
        for var1 in ['w']:
            data1_ = read_in_netcdf_fields(var1,fullpath_in).reshape((nx*ny,nz))
            for var2 in ['s']:
                data2_ = read_in_netcdf_fields(var2, fullpath_in).reshape((nx*ny,nz))
                for i in range(0,nz,5):
                    data[:,0] = data1_[:,i]
                    data[:,1] = data2_[:, i]

                    means, covariance = Gaussian_mixture_bivariate(data, var1, var2, np.int(d[0:-3]), i*dz)



    return


#----------------------------------------------------------------------
def Gaussian_mixture_univariate(data, var_name, time, z):
    for i in range(1):
        print('i', i)
        clf = mixture.GaussianMixture(n_components=2,covariance_type='full')
        clf.fit(data[:,i].reshape(nx*ny,1))

        n_sample = 100
        x_max = np.amax(data[:,i])
        x_min = np.amin(data[:,i])
        x = np.linspace(x_min,x_max,n_sample).reshape(n_sample,1)
        score = clf.score_samples(x)

        print(var_name + ': means=' + np.str(clf.means_))
        print(var_name + ': covar=' + np.str(clf.covariances_))

        plt.subplot(3,1,1)
        plt.hist(data)
        plt.title(var_name + ' (data), t='+str(time)+', z='+str(z))
        plt.subplot(3,1,2)
        plt.plot(x, np.exp(score))
        plt.plot([clf.means_[0], clf.means_[0]], [0, np.amax(np.exp(score))], 'k')
        plt.plot([clf.means_[1],clf.means_[1]],[0,np.amax(np.exp(score))],'k')
        plt.plot([clf.means_[0]-clf.covariances_[0,0], clf.means_[0]+clf.covariances_[0,0]], [np.amax(np.exp(score))/2, np.amax(np.exp(score))/2], 'k')
        plt.plot([clf.means_[1] - clf.covariances_[1, 0], clf.means_[1] + clf.covariances_[1, 0]],[np.amax(np.exp(score)) / 2, np.amax(np.exp(score)) / 2], 'k')
        plt.title('EM fit: likelihood')
        plt.subplot(3, 1, 3)
        plt.plot(x, score)
        # plt.title(np.str(clf.means_))
        plt.savefig('./figs_EM/EM_PDF_'+var_name+'_'+str(time)+'_z'+str(np.int(z))+'.png')
        plt.close()


    # lowest_bic = np.infty
    # bic = []
    # n_components_range = range(1, 7)
    # # cv_types = ['spherical', 'tied', 'diag', 'full']      # problem for 'tied'
    # cv_types = ['spherical', 'diag', 'full']
    # for cv_type in cv_types:
    #     print('cv type: ', cv_type)
    #     for n_components in n_components_range:
    #         print('n comp. ', n_components)
    #         '''mixture.GMM'''
    #         gmm = mixture.GaussianMixture(n_components=n_components,
    #                                       covariance_type=cv_type)
    #         '''fit'''
    #         gmm.fit(data)
    #         '''use BIC for avoiding under-/overfitting data'''
    #         bic.append(gmm.bic(data))
    #         if bic[-1] < lowest_bic:
    #             lowest_bic = bic[-1]
    #             best_gmm = gmm
    # bic = np.array(bic)  # convert list to array
    return clf.means_, clf.covariances_


#----------------------------------------------------------------------
def Gaussian_mixture_bivariate(data, var_name1, var_name2, time, z):
    clf = mixture.GaussianMixture(n_components=2,covariance_type='full')
    clf.fit(data)
    print('means=' + np.str(clf.means_))
    print('covar=' + np.str(clf.covariances_))

    # Plotting
    n_sample = 100
    x1_max = np.amax(data[:,0])
    x1_min = np.amin(data[:,0])
    x2_max = np.amax(data[:,1])
    x2_min = np.amin(data[:,1])
    x = np.linspace(x1_min,x1_max,n_sample)
    y = np.linspace(x2_min,x2_max,n_sample)
    X, Y = np.meshgrid(x,y)
    XX = np.array([X.ravel(),Y.ravel()]).T
    Z = clf.score_samples(XX).reshape(X.shape)

    plt.figure(figsize=(8,16))
    plt.subplot(3,1,1)
    plt.scatter(data[:,0], data[:,1], s=5, alpha=0.5)
    plt.title(var_name1 + var_name2 + ' (data), t='+str(time)+', z='+str(z))
    # ax1 = plt.contour(X,Y,np.exp(Z),levels=np.linspace(10,20,11))
    ax1 = plt.contour(X, Y, Z, levels=np.linspace(10, 20, 11))
    plt.colorbar(ax1,shrink=0.8)
    plt.xlabel('w [m/s]')
    plt.ylabel('entropy s [K]')
    plt.subplot(3,1,2)
    # ax1 = plt.contour(X,Y,np.exp(Z),levels=np.linspace(0,1,11))
    # plt.plot([clf.means_[0,0]],[clf.means_[0,1]], 'o', markersize=10)
    # plt.plot([clf.means_[1, 0]], [clf.means_[1, 1]], 'o', markersize=10)
    ax1 = plt.contourf(X,Y,np.exp(Z))
    plt.colorbar(ax1,shrink=0.8)
    plt.title('EM fit: likelihood')
    plt.xlabel('w [m/s]')
    plt.ylabel('entropy s [K]')
    plt.subplot(3, 1, 3)
    plt.scatter(data[:, 0], data[:, 1], s=5, alpha=0.4)
    ax1 = plt.contour(X, Y, np.exp(Z), levels=np.linspace(0, 1, 11), linewidths=3)
    plt.plot([clf.means_[0, 0]], [clf.means_[0, 1]], 'o', markersize=10)
    plt.plot([clf.means_[1, 0]], [clf.means_[1, 1]], 'o', markersize=10)
    plt.colorbar(ax1, shrink=0.8)
    # plt.title(var_name)
    # plt.title(np.str(clf.means_))
    plt.xlabel('w [m/s]')
    plt.ylabel('entropy s [K]')
    plt.savefig('./figs_EM/EM_PDF_bivariate_'+var_name1+'_'+var_name2+'_'+str(time)+'_z'+str(np.int(z))+'.png')
    plt.close()

    return clf.means_, clf.covariances_


# ____________________

def create_statistics_file(path,file_name):
    print('create file:', path)
    rootgrp = nc.Dataset(path+file_name+'.nc', 'w', format='NETCDF4')
    dimgrp = rootgrp.createGroup('dims')
    fieldgrp = rootgrp.createGroup('means')
    fieldgrp = rootgrp.createGroup('covariances')
    fieldgrp.createDimension('n',nx*ny*nz)
    fieldgrp.createDimension('nx', nx)
    fieldgrp.createDimension('ny', ny)
    fieldgrp.createDimension('nz', nz)
    rootgrp.close()
    print('create file end')
    return

def dump_variables(path, var_name, var):
    print('dump variables', path, var_name, var.shape)
    data = np.empty((n[0],n[1],n[2]),dtype=np.double,order='c')
    #        double[:] data = np.empty((Gr.dims.npl,), dtype=np.double, order='c')
    add_field(path, var_name)
    for i in range(0, n[0]):
        for j in range(0, n[1]):
            for k in range(0, n[2]):
                data[i,j,k] = var[i,j,k]
    write_field(path,var_name, data)
    return

def add_field(path, var_name):
    print('add field: ', var_name)
#    rootgrp = nc.Dataset(path, 'r+', format='NETCDF4')
    rootgrp = nc.Dataset(path, 'r+')
    fieldgrp = rootgrp.groups['fields']
    #    fieldgrp.createVariable(var_name, 'f8', ('n'))
    var = fieldgrp.createVariable(var_name, 'f8', ('nx', 'ny', 'nz'))
    rootgrp.close()
    return

def write_field(path, var_name, data):
    print('write field:', path, var_name, data.shape)
    rootgrp = nc.Dataset(path, 'r+', format='NETCDF4')
    fieldgrp = rootgrp.groups['fields']
    var = fieldgrp.variables[var_name]
    #    var[:] = np.array(data)
    var[:, :, :] = data
    rootgrp.close()
    return


#----------------------------------------------------------------------
def read_in_netcdf_fields(variable_name, fullpath_in):
    rootgrp = nc.Dataset(fullpath_in, 'r')
    var = rootgrp.groups['fields'].variables[variable_name]
    
    shape = var.shape
    # print('shape:',var.shape)
    data = np.ndarray(shape = var.shape)
    data = var[:]
    rootgrp.close()
    return data

#----------------------------------------------------------------------
def read_in(variable_name, group_name, fullpath_in):
    f = File(fullpath_in)
    
    #Get access to the profiles group
    profiles_group = f[group_name]
    #Get access to the variable dataset
    variable_dataset = profiles_group[variable_name]
    #Get the current shape of the dataset
    variable_dataset_shape = variable_dataset.shape
    
    variable = np.ndarray(shape = variable_dataset_shape)
    for t in range(variable_dataset_shape[0]):
        if group_name == "timeseries":
            variable[t] = variable_dataset[t]
        elif group_name == "profiles":
            variable[t,:] = variable_dataset[t, :]
        elif group_name == "correlations":
            variable[t,:] = variable_dataset[t, :]
        elif group_name == "fields":
            variable[t] = variable_dataset[t]

    f.close()
    return variable




if __name__ == "__main__":
    main()